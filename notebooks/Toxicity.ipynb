{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Install Dependencies and Obtain Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a dataset from Kaggle that has labelled comments with varying toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: pandas in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (5.29.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mercy/Desktop/ToxicCommentModel/toxic/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #for filepaths and folders\n",
    "import pandas as pd #read in tabular data for csvz\n",
    "import tensorflow as tf# deeplearning framework\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mercy/Desktop/ToxicCommentModel/notebooks'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join('/Users/mercy/Desktop/ToxicCommentModel/data','train.csv')) #read in the data in train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()#gives a short preview of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You, sir, are my hero. Any chance you remember what page that's on?\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4]['comment_text']#gives an example of one of the comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "Name: 4, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[2:]].iloc[4]#the labels of the comment in question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing:\n",
    "It converts the comments or words to numbers which are used as tokens for processing using textVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- -----------\n",
      "absl-py                      2.1.0\n",
      "appnope                      0.1.4\n",
      "asttokens                    3.0.0\n",
      "astunparse                   1.6.3\n",
      "certifi                      2024.12.14\n",
      "charset-normalizer           3.4.1\n",
      "comm                         0.2.2\n",
      "contourpy                    1.3.1\n",
      "cycler                       0.12.1\n",
      "debugpy                      1.8.11\n",
      "decorator                    5.1.1\n",
      "exceptiongroup               1.2.2\n",
      "executing                    2.1.0\n",
      "flatbuffers                  24.12.23\n",
      "fonttools                    4.55.3\n",
      "gast                         0.6.0\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.68.1\n",
      "h5py                         3.12.1\n",
      "idna                         3.10\n",
      "ipykernel                    6.29.5\n",
      "ipython                      8.31.0\n",
      "jedi                         0.19.2\n",
      "joblib                       1.4.2\n",
      "jupyter_client               8.6.3\n",
      "jupyter_core                 5.7.2\n",
      "keras                        3.7.0\n",
      "kiwisolver                   1.4.8\n",
      "libclang                     18.1.1\n",
      "Markdown                     3.7\n",
      "markdown-it-py               3.0.0\n",
      "MarkupSafe                   3.0.2\n",
      "matplotlib                   3.10.0\n",
      "matplotlib-inline            0.1.7\n",
      "mdurl                        0.1.2\n",
      "ml-dtypes                    0.4.1\n",
      "namex                        0.0.8\n",
      "nest-asyncio                 1.6.0\n",
      "numpy                        2.0.2\n",
      "opt_einsum                   3.4.0\n",
      "optree                       0.13.1\n",
      "packaging                    24.2\n",
      "pandas                       2.2.3\n",
      "parso                        0.8.4\n",
      "pexpect                      4.9.0\n",
      "pillow                       11.0.0\n",
      "pip                          24.3.1\n",
      "platformdirs                 4.3.6\n",
      "prompt_toolkit               3.0.48\n",
      "protobuf                     5.29.2\n",
      "psutil                       6.1.1\n",
      "ptyprocess                   0.7.0\n",
      "pure_eval                    0.2.3\n",
      "Pygments                     2.18.0\n",
      "pyparsing                    3.2.0\n",
      "python-dateutil              2.9.0.post0\n",
      "pytz                         2024.2\n",
      "pyzmq                        26.2.0\n",
      "requests                     2.32.3\n",
      "rich                         13.9.4\n",
      "scikit-learn                 1.6.0\n",
      "scipy                        1.14.1\n",
      "setuptools                   65.5.0\n",
      "six                          1.17.0\n",
      "stack-data                   0.6.3\n",
      "tensorboard                  2.18.0\n",
      "tensorboard-data-server      0.7.2\n",
      "tensorflow                   2.18.0\n",
      "tensorflow-io-gcs-filesystem 0.37.1\n",
      "termcolor                    2.5.0\n",
      "threadpoolctl                3.5.0\n",
      "tornado                      6.4.2\n",
      "traitlets                    5.14.3\n",
      "typing_extensions            4.12.2\n",
      "tzdata                       2024.2\n",
      "urllib3                      2.3.0\n",
      "wcwidth                      0.2.13\n",
      "Werkzeug                     3.1.3\n",
      "wheel                        0.45.1\n",
      "wrapt                        1.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization#to tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns#lists all the colum labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['comment_text']#comments\n",
    "y = df[df.columns[2:]].values#label ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[2:]].values#all our labels in a vector array form for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 200000#NUMBER OF WORDS IN THE VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=MAX_FEATURES,\n",
    "                               output_sequence_length=1800,#each sentence capped at 1800 word\n",
    "                               output_mode='int')#maps every word to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(X.values)#values converts it from a pandas series to a numpy array. trained it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([288, 263,   0,   0,   0])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of textvectorization\n",
    "vectorizer(\"Hello World\")[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " np.str_('the'),\n",
       " np.str_('to'),\n",
       " np.str_('of'),\n",
       " np.str_('and'),\n",
       " np.str_('a'),\n",
       " np.str_('you'),\n",
       " np.str_('i'),\n",
       " np.str_('is'),\n",
       " np.str_('that'),\n",
       " np.str_('in'),\n",
       " np.str_('it'),\n",
       " np.str_('for'),\n",
       " np.str_('this'),\n",
       " np.str_('not'),\n",
       " np.str_('on'),\n",
       " np.str_('be'),\n",
       " np.str_('as'),\n",
       " np.str_('have'),\n",
       " np.str_('are'),\n",
       " np.str_('your'),\n",
       " np.str_('with'),\n",
       " np.str_('if'),\n",
       " np.str_('article'),\n",
       " np.str_('was'),\n",
       " np.str_('or'),\n",
       " np.str_('but'),\n",
       " np.str_('page'),\n",
       " np.str_('my'),\n",
       " np.str_('an'),\n",
       " np.str_('from'),\n",
       " np.str_('by'),\n",
       " np.str_('do'),\n",
       " np.str_('at'),\n",
       " np.str_('about'),\n",
       " np.str_('me'),\n",
       " np.str_('so'),\n",
       " np.str_('wikipedia'),\n",
       " np.str_('can'),\n",
       " np.str_('what'),\n",
       " np.str_('there'),\n",
       " np.str_('all'),\n",
       " np.str_('has'),\n",
       " np.str_('will'),\n",
       " np.str_('talk'),\n",
       " np.str_('please'),\n",
       " np.str_('would'),\n",
       " np.str_('its'),\n",
       " np.str_('no'),\n",
       " np.str_('one'),\n",
       " np.str_('just'),\n",
       " np.str_('like'),\n",
       " np.str_('they'),\n",
       " np.str_('he'),\n",
       " np.str_('dont'),\n",
       " np.str_('which'),\n",
       " np.str_('any'),\n",
       " np.str_('been'),\n",
       " np.str_('should'),\n",
       " np.str_('more'),\n",
       " np.str_('we'),\n",
       " np.str_('some'),\n",
       " np.str_('other'),\n",
       " np.str_('who'),\n",
       " np.str_('see'),\n",
       " np.str_('here'),\n",
       " np.str_('also'),\n",
       " np.str_('his'),\n",
       " np.str_('think'),\n",
       " np.str_('im'),\n",
       " np.str_('because'),\n",
       " np.str_('know'),\n",
       " np.str_('how'),\n",
       " np.str_('am'),\n",
       " np.str_('people'),\n",
       " np.str_('why'),\n",
       " np.str_('edit'),\n",
       " np.str_('articles'),\n",
       " np.str_('only'),\n",
       " np.str_('out'),\n",
       " np.str_('up'),\n",
       " np.str_('when'),\n",
       " np.str_('were'),\n",
       " np.str_('use'),\n",
       " np.str_('then'),\n",
       " np.str_('may'),\n",
       " np.str_('time'),\n",
       " np.str_('did'),\n",
       " np.str_('them'),\n",
       " np.str_('now'),\n",
       " np.str_('being'),\n",
       " np.str_('their'),\n",
       " np.str_('than'),\n",
       " np.str_('thanks'),\n",
       " np.str_('even'),\n",
       " np.str_('get'),\n",
       " np.str_('make'),\n",
       " np.str_('good'),\n",
       " np.str_('had'),\n",
       " np.str_('very'),\n",
       " np.str_('information'),\n",
       " np.str_('does'),\n",
       " np.str_('could'),\n",
       " np.str_('well'),\n",
       " np.str_('want'),\n",
       " np.str_('such'),\n",
       " np.str_('sources'),\n",
       " np.str_('way'),\n",
       " np.str_('name'),\n",
       " np.str_('these'),\n",
       " np.str_('deletion'),\n",
       " np.str_('pages'),\n",
       " np.str_('first'),\n",
       " np.str_('help'),\n",
       " np.str_('new'),\n",
       " np.str_('editing'),\n",
       " np.str_('source'),\n",
       " np.str_('go'),\n",
       " np.str_('need'),\n",
       " np.str_('say'),\n",
       " np.str_('section'),\n",
       " np.str_('edits'),\n",
       " np.str_('again'),\n",
       " np.str_('thank'),\n",
       " np.str_('where'),\n",
       " np.str_('user'),\n",
       " np.str_('made'),\n",
       " np.str_('many'),\n",
       " np.str_('much'),\n",
       " np.str_('really'),\n",
       " np.str_('used'),\n",
       " np.str_('most'),\n",
       " np.str_('discussion'),\n",
       " np.str_('find'),\n",
       " np.str_('same'),\n",
       " np.str_('ive'),\n",
       " np.str_('deleted'),\n",
       " np.str_('into'),\n",
       " np.str_('fuck'),\n",
       " np.str_('those'),\n",
       " np.str_('work'),\n",
       " np.str_('since'),\n",
       " np.str_('before'),\n",
       " np.str_('after'),\n",
       " np.str_('point'),\n",
       " np.str_('add'),\n",
       " np.str_('look'),\n",
       " np.str_('right'),\n",
       " np.str_('read'),\n",
       " np.str_('image'),\n",
       " np.str_('take'),\n",
       " np.str_('still'),\n",
       " np.str_('over'),\n",
       " np.str_('someone'),\n",
       " np.str_('him'),\n",
       " np.str_('two'),\n",
       " np.str_('back'),\n",
       " np.str_('too'),\n",
       " np.str_('fact'),\n",
       " np.str_('link'),\n",
       " np.str_('said'),\n",
       " np.str_('own'),\n",
       " np.str_('something'),\n",
       " np.str_('going'),\n",
       " np.str_('youre'),\n",
       " np.str_('blocked'),\n",
       " np.str_('list'),\n",
       " np.str_('stop'),\n",
       " np.str_('without'),\n",
       " np.str_('content'),\n",
       " np.str_('hi'),\n",
       " np.str_('under'),\n",
       " np.str_('editors'),\n",
       " np.str_('our'),\n",
       " np.str_('block'),\n",
       " np.str_('thats'),\n",
       " np.str_('us'),\n",
       " np.str_('added'),\n",
       " np.str_('utc'),\n",
       " np.str_('history'),\n",
       " np.str_('another'),\n",
       " np.str_('doesnt'),\n",
       " np.str_('removed'),\n",
       " np.str_('might'),\n",
       " np.str_('note'),\n",
       " np.str_('however'),\n",
       " np.str_('sure'),\n",
       " np.str_('place'),\n",
       " np.str_('never'),\n",
       " np.str_('done'),\n",
       " np.str_('welcome'),\n",
       " np.str_('her'),\n",
       " np.str_('case'),\n",
       " np.str_('put'),\n",
       " np.str_('personal'),\n",
       " np.str_('seems'),\n",
       " np.str_('reason'),\n",
       " np.str_('better'),\n",
       " np.str_('using'),\n",
       " np.str_('yourself'),\n",
       " np.str_('cant'),\n",
       " np.str_('actually'),\n",
       " np.str_('ask'),\n",
       " np.str_('comment'),\n",
       " np.str_('while'),\n",
       " np.str_('vandalism'),\n",
       " np.str_('feel'),\n",
       " np.str_('question'),\n",
       " np.str_('anything'),\n",
       " np.str_('believe'),\n",
       " np.str_('person'),\n",
       " np.str_('links'),\n",
       " np.str_('things'),\n",
       " np.str_('both'),\n",
       " np.str_('didnt'),\n",
       " np.str_('comments'),\n",
       " np.str_('best'),\n",
       " np.str_('ill'),\n",
       " np.str_('part'),\n",
       " np.str_('she'),\n",
       " np.str_('hope'),\n",
       " np.str_('policy'),\n",
       " np.str_('against'),\n",
       " np.str_('off'),\n",
       " np.str_('keep'),\n",
       " np.str_('already'),\n",
       " np.str_('free'),\n",
       " np.str_('wiki'),\n",
       " np.str_('thing'),\n",
       " np.str_('nothing'),\n",
       " np.str_('change'),\n",
       " np.str_('wrong'),\n",
       " np.str_('though'),\n",
       " np.str_('problem'),\n",
       " np.str_('remove'),\n",
       " np.str_('little'),\n",
       " np.str_('subject'),\n",
       " np.str_('•'),\n",
       " np.str_('others'),\n",
       " np.str_('trying'),\n",
       " np.str_('tag'),\n",
       " np.str_('copyright'),\n",
       " np.str_('must'),\n",
       " np.str_('understand'),\n",
       " np.str_('above'),\n",
       " np.str_('few'),\n",
       " np.str_('anyone'),\n",
       " np.str_('speedy'),\n",
       " np.str_('last'),\n",
       " np.str_('issue'),\n",
       " np.str_('give'),\n",
       " np.str_('questions'),\n",
       " np.str_('agree'),\n",
       " np.str_('rather'),\n",
       " np.str_('years'),\n",
       " np.str_('let'),\n",
       " np.str_('2'),\n",
       " np.str_('different'),\n",
       " np.str_('editor'),\n",
       " np.str_('long'),\n",
       " np.str_('reliable'),\n",
       " np.str_('making'),\n",
       " np.str_('world'),\n",
       " np.str_('come'),\n",
       " np.str_('sorry'),\n",
       " np.str_('isnt'),\n",
       " np.str_('reference'),\n",
       " np.str_('mean'),\n",
       " np.str_('continue'),\n",
       " np.str_('try'),\n",
       " np.str_('references'),\n",
       " np.str_('found'),\n",
       " np.str_('doing'),\n",
       " np.str_('text'),\n",
       " np.str_('great'),\n",
       " np.str_('leave'),\n",
       " np.str_('says'),\n",
       " np.str_('got'),\n",
       " np.str_('probably'),\n",
       " np.str_('english'),\n",
       " np.str_('original'),\n",
       " np.str_('every'),\n",
       " np.str_('1'),\n",
       " np.str_('simply'),\n",
       " np.str_('word'),\n",
       " np.str_('users'),\n",
       " np.str_('fair'),\n",
       " np.str_('hello'),\n",
       " np.str_('either'),\n",
       " np.str_('check'),\n",
       " np.str_('least'),\n",
       " np.str_('adding'),\n",
       " np.str_('ip'),\n",
       " np.str_('show'),\n",
       " np.str_('site'),\n",
       " np.str_('state'),\n",
       " np.str_('else'),\n",
       " np.str_('delete'),\n",
       " np.str_('consensus'),\n",
       " np.str_('enough'),\n",
       " np.str_('request'),\n",
       " np.str_('far'),\n",
       " np.str_('opinion'),\n",
       " np.str_('created'),\n",
       " np.str_('around'),\n",
       " np.str_('life'),\n",
       " np.str_('day'),\n",
       " np.str_('between'),\n",
       " np.str_('through'),\n",
       " np.str_('example'),\n",
       " np.str_('view'),\n",
       " np.str_('yes'),\n",
       " np.str_('reverted'),\n",
       " np.str_('yet'),\n",
       " np.str_('etc'),\n",
       " np.str_('id'),\n",
       " np.str_('matter'),\n",
       " np.str_('shit'),\n",
       " np.str_('u'),\n",
       " np.str_('war'),\n",
       " np.str_('notable'),\n",
       " np.str_('contributions'),\n",
       " np.str_('given'),\n",
       " np.str_('thought'),\n",
       " np.str_('material'),\n",
       " np.str_('book'),\n",
       " np.str_('admin'),\n",
       " np.str_('write'),\n",
       " np.str_('post'),\n",
       " np.str_('down'),\n",
       " np.str_('account'),\n",
       " np.str_('clearly'),\n",
       " np.str_('having'),\n",
       " np.str_('encyclopedia'),\n",
       " np.str_('lot'),\n",
       " np.str_('support'),\n",
       " np.str_('real'),\n",
       " np.str_('bad'),\n",
       " np.str_('message'),\n",
       " np.str_('needs'),\n",
       " np.str_('images'),\n",
       " np.str_('tell'),\n",
       " np.str_('seem'),\n",
       " np.str_('called'),\n",
       " np.str_('maybe'),\n",
       " np.str_('evidence'),\n",
       " np.str_('instead'),\n",
       " np.str_('ever'),\n",
       " np.str_('3'),\n",
       " np.str_('correct'),\n",
       " np.str_('saying'),\n",
       " np.str_('clear'),\n",
       " np.str_('always'),\n",
       " np.str_('number'),\n",
       " np.str_('important'),\n",
       " np.str_('further'),\n",
       " np.str_('quite'),\n",
       " np.str_('perhaps'),\n",
       " np.str_('old'),\n",
       " np.str_('—'),\n",
       " np.str_('true'),\n",
       " np.str_('until'),\n",
       " np.str_('hate'),\n",
       " np.str_('states'),\n",
       " np.str_('whether'),\n",
       " np.str_('consider'),\n",
       " np.str_('written'),\n",
       " np.str_('claim'),\n",
       " np.str_('language'),\n",
       " np.str_('media'),\n",
       " np.str_('bit'),\n",
       " np.str_('once'),\n",
       " np.str_('guidelines'),\n",
       " np.str_('term'),\n",
       " np.str_('criteria'),\n",
       " np.str_('research'),\n",
       " np.str_('nigger'),\n",
       " np.str_('version'),\n",
       " np.str_('times'),\n",
       " np.str_('website'),\n",
       " np.str_('getting'),\n",
       " np.str_('fucking'),\n",
       " np.str_('theres'),\n",
       " np.str_('review'),\n",
       " np.str_('mention'),\n",
       " np.str_('pov'),\n",
       " np.str_('oh'),\n",
       " np.str_('makes'),\n",
       " np.str_('several'),\n",
       " np.str_('revert'),\n",
       " np.str_('considered'),\n",
       " np.str_('changes'),\n",
       " np.str_('cannot'),\n",
       " np.str_('words'),\n",
       " np.str_('idea'),\n",
       " np.str_('title'),\n",
       " np.str_('suck'),\n",
       " np.str_('address'),\n",
       " np.str_('notice'),\n",
       " np.str_('based'),\n",
       " np.str_('top'),\n",
       " np.str_('following'),\n",
       " np.str_('current'),\n",
       " np.str_('each'),\n",
       " np.str_('listed'),\n",
       " np.str_('means'),\n",
       " np.str_('possible'),\n",
       " np.str_('group'),\n",
       " np.str_('facts'),\n",
       " np.str_('regarding'),\n",
       " np.str_('care'),\n",
       " np.str_('rules'),\n",
       " np.str_('second'),\n",
       " np.str_('main'),\n",
       " np.str_('template'),\n",
       " np.str_('mentioned'),\n",
       " np.str_('general'),\n",
       " np.str_('year'),\n",
       " np.str_('attack'),\n",
       " np.str_('kind'),\n",
       " np.str_('whole'),\n",
       " np.str_('course'),\n",
       " np.str_('statement'),\n",
       " np.str_('left'),\n",
       " np.str_('hey'),\n",
       " np.str_('date'),\n",
       " np.str_('include'),\n",
       " np.str_('seen'),\n",
       " np.str_('three'),\n",
       " np.str_('issues'),\n",
       " np.str_('start'),\n",
       " np.str_('ass'),\n",
       " np.str_('ok'),\n",
       " np.str_('end'),\n",
       " np.str_('wikipedias'),\n",
       " np.str_('call'),\n",
       " np.str_('less'),\n",
       " np.str_('topic'),\n",
       " np.str_('gay'),\n",
       " np.str_('suggest'),\n",
       " np.str_('man'),\n",
       " np.str_('including'),\n",
       " np.str_('happy'),\n",
       " np.str_('sense'),\n",
       " np.str_('provide'),\n",
       " np.str_('create'),\n",
       " np.str_('big'),\n",
       " np.str_('days'),\n",
       " np.str_('myself'),\n",
       " np.str_('american'),\n",
       " np.str_('redirect'),\n",
       " np.str_('known'),\n",
       " np.str_('sentence'),\n",
       " np.str_('move'),\n",
       " np.str_('appropriate'),\n",
       " np.str_('changed'),\n",
       " np.str_('love'),\n",
       " np.str_('notability'),\n",
       " np.str_('explain'),\n",
       " np.str_('started'),\n",
       " np.str_('included'),\n",
       " np.str_('removing'),\n",
       " np.str_('project'),\n",
       " np.str_('anyway'),\n",
       " np.str_('info'),\n",
       " np.str_('mind'),\n",
       " np.str_('school'),\n",
       " np.str_('2005'),\n",
       " np.str_('next'),\n",
       " np.str_('looking'),\n",
       " np.str_('although'),\n",
       " np.str_('picture'),\n",
       " np.str_('relevant'),\n",
       " np.str_('four'),\n",
       " np.str_('die'),\n",
       " np.str_('sign'),\n",
       " np.str_('answer'),\n",
       " np.str_('style'),\n",
       " np.str_('away'),\n",
       " np.str_('per'),\n",
       " np.str_('order'),\n",
       " np.str_('warning'),\n",
       " np.str_('wont'),\n",
       " np.str_('recent'),\n",
       " np.str_('youve'),\n",
       " np.str_('interest'),\n",
       " np.str_('community'),\n",
       " np.str_('summary'),\n",
       " np.str_('later'),\n",
       " np.str_('lol'),\n",
       " np.str_('claims'),\n",
       " np.str_('currently'),\n",
       " np.str_('discuss'),\n",
       " np.str_('interested'),\n",
       " np.str_('policies'),\n",
       " np.str_('attacks'),\n",
       " np.str_('especially'),\n",
       " np.str_('wish'),\n",
       " np.str_('wrote'),\n",
       " np.str_('able'),\n",
       " np.str_('specific'),\n",
       " np.str_('public'),\n",
       " np.str_('taken'),\n",
       " np.str_('writing'),\n",
       " np.str_('neutral'),\n",
       " np.str_('full'),\n",
       " np.str_('names'),\n",
       " np.str_('within'),\n",
       " np.str_('4'),\n",
       " np.str_('position'),\n",
       " np.str_('related'),\n",
       " np.str_('below'),\n",
       " np.str_('line'),\n",
       " np.str_('wanted'),\n",
       " np.str_('during'),\n",
       " np.str_('appears'),\n",
       " np.str_('stuff'),\n",
       " np.str_('certainly'),\n",
       " np.str_('official'),\n",
       " np.str_('nice'),\n",
       " np.str_('itself'),\n",
       " np.str_('faith'),\n",
       " np.str_('everyone'),\n",
       " np.str_('wasnt'),\n",
       " np.str_('live'),\n",
       " np.str_('report'),\n",
       " np.str_('completely'),\n",
       " np.str_('according'),\n",
       " np.str_('unless'),\n",
       " np.str_('common'),\n",
       " np.str_('pretty'),\n",
       " np.str_('country'),\n",
       " np.str_('everything'),\n",
       " np.str_('looks'),\n",
       " np.str_('due'),\n",
       " np.str_('single'),\n",
       " np.str_('hes'),\n",
       " np.str_('process'),\n",
       " np.str_('contribs'),\n",
       " np.str_('news'),\n",
       " np.str_('involved'),\n",
       " np.str_('god'),\n",
       " np.str_('fat'),\n",
       " np.str_('therefore'),\n",
       " np.str_('obviously'),\n",
       " np.str_('remember'),\n",
       " np.str_('lead'),\n",
       " np.str_('hard'),\n",
       " np.str_('admins'),\n",
       " np.str_('came'),\n",
       " np.str_('edited'),\n",
       " np.str_('web'),\n",
       " np.str_('stay'),\n",
       " np.str_('learn'),\n",
       " np.str_('response'),\n",
       " np.str_('future'),\n",
       " np.str_('past'),\n",
       " np.str_('asked'),\n",
       " np.str_('truth'),\n",
       " np.str_('reading'),\n",
       " np.str_('power'),\n",
       " np.str_('2006'),\n",
       " np.str_('stupid'),\n",
       " np.str_('entry'),\n",
       " np.str_('quote'),\n",
       " np.str_('posted'),\n",
       " np.str_('nor'),\n",
       " np.str_('talking'),\n",
       " np.str_('placed'),\n",
       " np.str_('5'),\n",
       " np.str_('ago'),\n",
       " np.str_('similar'),\n",
       " np.str_('email'),\n",
       " np.str_('game'),\n",
       " np.str_('published'),\n",
       " np.str_('exactly'),\n",
       " np.str_('today'),\n",
       " np.str_('reasons'),\n",
       " np.str_('paragraph'),\n",
       " np.str_('faggot'),\n",
       " np.str_('city'),\n",
       " np.str_('argument'),\n",
       " np.str_('whatever'),\n",
       " np.str_('system'),\n",
       " np.str_('working'),\n",
       " np.str_('false'),\n",
       " np.str_('sandbox'),\n",
       " np.str_('moron'),\n",
       " np.str_('political'),\n",
       " np.str_('noticed'),\n",
       " np.str_('useful'),\n",
       " np.str_('havent'),\n",
       " np.str_('guy'),\n",
       " np.str_('high'),\n",
       " np.str_('regards'),\n",
       " np.str_('united'),\n",
       " np.str_('guess'),\n",
       " np.str_('appreciate'),\n",
       " np.str_('particular'),\n",
       " np.str_('deleting'),\n",
       " np.str_('form'),\n",
       " np.str_('books'),\n",
       " np.str_('government'),\n",
       " np.str_('dispute'),\n",
       " np.str_('five'),\n",
       " np.str_('british'),\n",
       " np.str_('reverting'),\n",
       " np.str_('major'),\n",
       " np.str_('problems'),\n",
       " np.str_('national'),\n",
       " np.str_('party'),\n",
       " np.str_('provided'),\n",
       " np.str_('often'),\n",
       " np.str_('ones'),\n",
       " np.str_('become'),\n",
       " np.str_('lets'),\n",
       " np.str_('tried'),\n",
       " np.str_('side'),\n",
       " np.str_('administrator'),\n",
       " np.str_('along'),\n",
       " np.str_('reply'),\n",
       " np.str_('almost'),\n",
       " np.str_('needed'),\n",
       " np.str_('stated'),\n",
       " np.str_('rule'),\n",
       " np.str_('took'),\n",
       " np.str_('search'),\n",
       " np.str_('knowledge'),\n",
       " np.str_('banned'),\n",
       " np.str_('cheers'),\n",
       " np.str_('taking'),\n",
       " np.str_('vandalize'),\n",
       " np.str_('–'),\n",
       " np.str_('certain'),\n",
       " np.str_('2007'),\n",
       " np.str_('username'),\n",
       " np.str_('fine'),\n",
       " np.str_('status'),\n",
       " np.str_('law'),\n",
       " np.str_('points'),\n",
       " np.str_('company'),\n",
       " np.str_('otherwise'),\n",
       " np.str_('uploaded'),\n",
       " np.str_('terms'),\n",
       " np.str_('explanation'),\n",
       " np.str_('generally'),\n",
       " np.str_('sort'),\n",
       " np.str_('entire'),\n",
       " np.str_('shows'),\n",
       " np.str_('description'),\n",
       " np.str_('whats'),\n",
       " np.str_('recently'),\n",
       " np.str_('follow'),\n",
       " np.str_('guys'),\n",
       " np.str_('2008'),\n",
       " np.str_('likely'),\n",
       " np.str_('film'),\n",
       " np.str_('present'),\n",
       " np.str_('aware'),\n",
       " np.str_('saw'),\n",
       " np.str_('definition'),\n",
       " np.str_('cited'),\n",
       " np.str_('alone'),\n",
       " np.str_('google'),\n",
       " np.str_('music'),\n",
       " np.str_('soon'),\n",
       " np.str_('indeed'),\n",
       " np.str_('decide'),\n",
       " np.str_('ban'),\n",
       " np.str_('wp'),\n",
       " np.str_('appear'),\n",
       " np.str_('views'),\n",
       " np.str_('week'),\n",
       " np.str_('open'),\n",
       " np.str_('citation'),\n",
       " np.str_('contributing'),\n",
       " np.str_('actual'),\n",
       " np.str_('set'),\n",
       " np.str_('interesting'),\n",
       " np.str_('piece'),\n",
       " np.str_('c'),\n",
       " np.str_('short'),\n",
       " np.str_('white'),\n",
       " np.str_('told'),\n",
       " np.str_('theory'),\n",
       " np.str_('area'),\n",
       " np.str_('improve'),\n",
       " np.str_('external'),\n",
       " np.str_('small'),\n",
       " np.str_('story'),\n",
       " np.str_('contact'),\n",
       " np.str_('simple'),\n",
       " np.str_('2004'),\n",
       " np.str_('various'),\n",
       " np.str_('allowed'),\n",
       " np.str_('moved'),\n",
       " np.str_('test'),\n",
       " np.str_('internet'),\n",
       " np.str_('obvious'),\n",
       " np.str_('family'),\n",
       " np.str_('band'),\n",
       " np.str_('attention'),\n",
       " np.str_('arent'),\n",
       " np.str_('proposed'),\n",
       " np.str_('jew'),\n",
       " np.str_('themselves'),\n",
       " np.str_('members'),\n",
       " np.str_('wouldnt'),\n",
       " np.str_('result'),\n",
       " np.str_('disagree'),\n",
       " np.str_('thus'),\n",
       " np.str_('cunt'),\n",
       " np.str_('went'),\n",
       " np.str_('type'),\n",
       " np.str_('sites'),\n",
       " np.str_('ie'),\n",
       " np.str_('context'),\n",
       " np.str_('mr'),\n",
       " np.str_('previous'),\n",
       " np.str_('nonsense'),\n",
       " np.str_('actions'),\n",
       " np.str_('tags'),\n",
       " np.str_('cite'),\n",
       " np.str_('works'),\n",
       " np.str_('10'),\n",
       " np.str_('citations'),\n",
       " np.str_('jews'),\n",
       " np.str_('university'),\n",
       " np.str_('re'),\n",
       " np.str_('enjoy'),\n",
       " np.str_('conflict'),\n",
       " np.str_('hours'),\n",
       " np.str_('shouldnt'),\n",
       " np.str_('proper'),\n",
       " np.str_('bias'),\n",
       " np.str_('category'),\n",
       " np.str_('job'),\n",
       " np.str_('longer'),\n",
       " np.str_('file'),\n",
       " np.str_('together'),\n",
       " np.str_('hell'),\n",
       " np.str_('sourced'),\n",
       " np.str_('sucks'),\n",
       " np.str_('addition'),\n",
       " np.str_('happened'),\n",
       " np.str_('avoid'),\n",
       " np.str_('automatically'),\n",
       " np.str_('author'),\n",
       " np.str_('valid'),\n",
       " np.str_('black'),\n",
       " np.str_('creating'),\n",
       " np.str_('deal'),\n",
       " np.str_('worked'),\n",
       " np.str_('npov'),\n",
       " np.str_('goes'),\n",
       " np.str_('himself'),\n",
       " np.str_('seriously'),\n",
       " np.str_('john'),\n",
       " np.str_('death'),\n",
       " np.str_('proof'),\n",
       " np.str_('respect'),\n",
       " np.str_('bitch'),\n",
       " np.str_('science'),\n",
       " np.str_('human'),\n",
       " np.str_('biased'),\n",
       " np.str_('comes'),\n",
       " np.str_('helpful'),\n",
       " np.str_('large'),\n",
       " np.str_('accepted'),\n",
       " np.str_('available'),\n",
       " np.str_('exist'),\n",
       " np.str_('series'),\n",
       " np.str_('tildes'),\n",
       " np.str_('opinions'),\n",
       " np.str_('hand'),\n",
       " np.str_('6'),\n",
       " np.str_('indicate'),\n",
       " np.str_('sections'),\n",
       " np.str_('rights'),\n",
       " np.str_('necessary'),\n",
       " np.str_('act'),\n",
       " np.str_('meaning'),\n",
       " np.str_('attempt'),\n",
       " np.str_('accept'),\n",
       " np.str_('personally'),\n",
       " np.str_('statements'),\n",
       " np.str_('violation'),\n",
       " np.str_('months'),\n",
       " np.str_('criticism'),\n",
       " np.str_('accurate'),\n",
       " np.str_('action'),\n",
       " np.str_('usually'),\n",
       " np.str_('unblock'),\n",
       " np.str_('german'),\n",
       " np.str_('pig'),\n",
       " np.str_('cause'),\n",
       " np.str_('yeah'),\n",
       " np.str_('living'),\n",
       " np.str_('copy'),\n",
       " np.str_('debate'),\n",
       " np.str_('upon'),\n",
       " np.str_('assume'),\n",
       " np.str_('july'),\n",
       " np.str_('calling'),\n",
       " np.str_('standard'),\n",
       " np.str_('video'),\n",
       " np.str_('play'),\n",
       " np.str_('rest'),\n",
       " np.str_('tagged'),\n",
       " np.str_('doubt'),\n",
       " np.str_('sex'),\n",
       " np.str_('multiple'),\n",
       " np.str_('theyre'),\n",
       " np.str_('historical'),\n",
       " np.str_('serious'),\n",
       " np.str_('details'),\n",
       " np.str_('dick'),\n",
       " np.str_('youll'),\n",
       " np.str_('separate'),\n",
       " np.str_('manual'),\n",
       " np.str_('record'),\n",
       " np.str_('blocking'),\n",
       " np.str_('afd'),\n",
       " np.str_('explaining'),\n",
       " np.str_('situation'),\n",
       " np.str_('refer'),\n",
       " np.str_('wikiproject'),\n",
       " np.str_('heard'),\n",
       " np.str_('online'),\n",
       " np.str_('level'),\n",
       " np.str_('fix'),\n",
       " np.str_('asking'),\n",
       " np.str_('7'),\n",
       " np.str_('complete'),\n",
       " np.str_('speak'),\n",
       " np.str_('lack'),\n",
       " np.str_('messages'),\n",
       " np.str_('none'),\n",
       " np.str_('prove'),\n",
       " np.str_('third'),\n",
       " np.str_('subjects'),\n",
       " np.str_('church'),\n",
       " np.str_('apparently'),\n",
       " np.str_('2009'),\n",
       " np.str_('south'),\n",
       " np.str_('rationale'),\n",
       " np.str_('bullshit'),\n",
       " np.str_('data'),\n",
       " np.str_('directly'),\n",
       " np.str_('august'),\n",
       " np.str_('period'),\n",
       " np.str_('legal'),\n",
       " np.str_('behavior'),\n",
       " np.str_('difference'),\n",
       " np.str_('contribute'),\n",
       " np.str_('greek'),\n",
       " np.str_('huge'),\n",
       " np.str_('gets'),\n",
       " np.str_('wikipedian'),\n",
       " np.str_('couple'),\n",
       " np.str_('supposed'),\n",
       " np.str_('among'),\n",
       " np.str_('early'),\n",
       " np.str_('except'),\n",
       " np.str_('march'),\n",
       " np.str_('close'),\n",
       " np.str_('quality'),\n",
       " np.str_('space'),\n",
       " np.str_('meant'),\n",
       " np.str_('countries'),\n",
       " np.str_('run'),\n",
       " np.str_('team'),\n",
       " np.str_('uses'),\n",
       " np.str_('military'),\n",
       " np.str_('b'),\n",
       " np.str_('changing'),\n",
       " np.str_('existing'),\n",
       " np.str_('specifically'),\n",
       " np.str_('significant'),\n",
       " np.str_('2010'),\n",
       " np.str_('pillars'),\n",
       " np.str_('fish'),\n",
       " np.str_('incorrect'),\n",
       " np.str_('culture'),\n",
       " np.str_('described'),\n",
       " np.str_('produce'),\n",
       " np.str_('jewish'),\n",
       " np.str_('24'),\n",
       " np.str_('uk'),\n",
       " np.str_('disruptive'),\n",
       " np.str_('d'),\n",
       " np.str_('field'),\n",
       " np.str_('error'),\n",
       " np.str_('india'),\n",
       " np.str_('head'),\n",
       " np.str_('primary'),\n",
       " np.str_('friend'),\n",
       " np.str_('earlier'),\n",
       " np.str_('sometimes'),\n",
       " np.str_('outside'),\n",
       " np.str_('20'),\n",
       " np.str_('purpose'),\n",
       " np.str_('administrators'),\n",
       " np.str_('modern'),\n",
       " np.str_('photo'),\n",
       " np.str_('table'),\n",
       " np.str_('particularly'),\n",
       " np.str_('t'),\n",
       " np.str_('release'),\n",
       " np.str_('gave'),\n",
       " np.str_('box'),\n",
       " np.str_('cases'),\n",
       " np.str_('inclusion'),\n",
       " np.str_('born'),\n",
       " np.str_('pictures'),\n",
       " np.str_('readers'),\n",
       " np.str_('june'),\n",
       " np.str_('character'),\n",
       " np.str_('vote'),\n",
       " np.str_('okay'),\n",
       " np.str_('groups'),\n",
       " np.str_('anonymous'),\n",
       " np.str_('abuse'),\n",
       " np.str_('arguments'),\n",
       " np.str_('business'),\n",
       " np.str_('shall'),\n",
       " np.str_('sock'),\n",
       " np.str_('tutorial'),\n",
       " np.str_('january'),\n",
       " np.str_('friends'),\n",
       " np.str_('numbers'),\n",
       " np.str_('control'),\n",
       " np.str_('thinking'),\n",
       " np.str_('member'),\n",
       " np.str_('linked'),\n",
       " np.str_('happen'),\n",
       " np.str_('reported'),\n",
       " np.str_('contest'),\n",
       " np.str_('coming'),\n",
       " np.str_('takes'),\n",
       " np.str_('concerns'),\n",
       " np.str_('allow'),\n",
       " np.str_('wait'),\n",
       " np.str_('majority'),\n",
       " np.str_('giving'),\n",
       " np.str_('8'),\n",
       " np.str_('bring'),\n",
       " np.str_('eg'),\n",
       " np.str_('worth'),\n",
       " np.str_('kill'),\n",
       " np.str_('totally'),\n",
       " np.str_('red'),\n",
       " np.str_('force'),\n",
       " np.str_('decided'),\n",
       " np.str_('discussed'),\n",
       " np.str_('house'),\n",
       " np.str_('finally'),\n",
       " np.str_('absolutely'),\n",
       " np.str_('putting'),\n",
       " np.str_('scientific'),\n",
       " np.str_('respond'),\n",
       " np.str_('mistake'),\n",
       " np.str_('decision'),\n",
       " np.str_('de'),\n",
       " np.str_('lost'),\n",
       " np.str_('entirely'),\n",
       " np.str_('100'),\n",
       " np.str_('towards'),\n",
       " np.str_('merely'),\n",
       " np.str_('home'),\n",
       " np.str_('neither'),\n",
       " np.str_('dear'),\n",
       " np.str_('independent'),\n",
       " np.str_('international'),\n",
       " np.str_('song'),\n",
       " np.str_('balls'),\n",
       " np.str_('wants'),\n",
       " np.str_('possibly'),\n",
       " np.str_('unsigned'),\n",
       " np.str_('million'),\n",
       " np.str_('irrelevant'),\n",
       " np.str_('standards'),\n",
       " np.str_('april'),\n",
       " np.str_('12'),\n",
       " np.str_('press'),\n",
       " np.str_('figure'),\n",
       " np.str_('organization'),\n",
       " np.str_('looked'),\n",
       " np.str_('inappropriate'),\n",
       " np.str_('chance'),\n",
       " np.str_('posting'),\n",
       " np.str_('population'),\n",
       " np.str_('advice'),\n",
       " np.str_('posts'),\n",
       " np.str_('north'),\n",
       " np.str_('events'),\n",
       " np.str_('unfortunately'),\n",
       " np.str_('named'),\n",
       " np.str_('album'),\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = vectorizer(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(159571, 1800), dtype=int64, numpy=\n",
       "array([[  645,    76,     2, ...,     0,     0,     0],\n",
       "       [    1,    54,  2489, ...,     0,     0,     0],\n",
       "       [  425,   441,    70, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [32445,  7392,   383, ...,     0,     0,     0],\n",
       "       [    5,    12,   534, ...,     0,     0,     0],\n",
       "       [    5,     8,   130, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Tensorflow Dataset pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,y))#parse data\n",
    "dataset = dataset.cache()#\n",
    "dataset = dataset.shuffle(160000)#shuffles\n",
    "dataset = dataset.batch(16)#data is in batches of 16 samples\n",
    "dataset = dataset.prefetch(8) #prevent bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X, batch_Y = dataset.as_numpy_iterator().next()#fetches the next batch(x is the comments and y is the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training, data and test partitions #returns in form of batches\n",
    "train = dataset.take(int(len(dataset)*.7)) #7 training\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2)) #20 percent validation\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))#10 percent testing\n",
    "                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 312,    4,  422, ...,    0,    0,    0],\n",
       "        [1949,  410,   14, ...,    0,    0,    0],\n",
       "        [  82,   20,    7, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [1909,   60, 1909, ...,    0,    0,    0],\n",
       "        [5088, 2668,    5, ...,    0,    0,    0],\n",
       "        [  34,    2,  401, ...,    0,    0,    0]]),\n",
       " array([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create Sequential Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 6)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()#Instantiate our model\n",
    "model.add(Embedding(MAX_FEATURES+1, 32))#embedding layer -- no of words + 1, 1 embedding per word \n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))#no of LSTM layers and we specify it should be bidirectional nad LSTM requires activation of tanh\n",
    "model.add(Dense(128, activation='relu'))#three feature extraction layers for embedding \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#no of different outputs or labels \n",
    "model.add(Dense(6, activation='sigmoid'))#converts outputs to between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam')#BinaryCrossentropy for multiple output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()#look at our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train, epochs=5, validation_data=val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
